{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zHbwqt77Nx4"
      },
      "outputs": [],
      "source": [
        "#First step :- Load libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense      #neural net layers\n",
        "import pandas as pd                  \n",
        "from sklearn.model_selection import train_test_split   #to spit dataset into train,validation,test \n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install -q xlrd           #library to read from excel sheet\n",
        "import io                      #input-output = io\n",
        "from google.colab import files #important to locally save files\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth    #to use Google Cloud SDK\n",
        "from pydrive.drive import GoogleDrive  \n",
        "from google.colab import auth           #to authenticate gmail ID\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()   #to authenticate gmail ID in Google Cloud SDK\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "import random\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "import numpy as np\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1K5tKWu7Tlk"
      },
      "outputs": [],
      "source": [
        "BIG_SCORE = 1.e6  # type: float\n",
        "\n",
        "class Particle:\n",
        "    def __init__(self, model, params):\n",
        "        self.model = model\n",
        "        self.params = params\n",
        "        self.init_weights = model.get_weights()\n",
        "        self.velocities = [None] * len(self.init_weights)\n",
        "        self.length = len(self.init_weights)\n",
        "        for i, layer in enumerate(self.init_weights):\n",
        "            self.velocities[i] = np.random.rand(*layer.shape) / 5 - 0.10\n",
        "            #self.velocities[i] = np.zeros(layer.shape)\n",
        "\n",
        "        self.best_weights = []\n",
        "        self.best_score = BIG_SCORE\n",
        "\n",
        "    def get_score(self, x, y, update=True):\n",
        "        local_score = self.model.evaluate(x, y, verbose=0)\n",
        "        if local_score < self.best_score and update:\n",
        "            self.best_score = local_score\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "        return local_score\n",
        "\n",
        "    def _update_velocities(self, global_best_weights, depth):\n",
        "        new_velocities = [None] * len(self.init_weights)\n",
        "        weights = self.model.get_weights()\n",
        "        local_rand, global_rand = random.random(), random.random()\n",
        "\n",
        "        for i, layer in enumerate(weights):\n",
        "            if i >= depth:\n",
        "              new_velocities[i] = self.velocities[i]\n",
        "              continue\n",
        "            new_v = self.params['acc'] * self.velocities[i]\n",
        "            new_v = new_v + self.params['local_acc'] * local_rand * (self.best_weights[i] - layer)\n",
        "            new_v = new_v + self.params['global_acc'] * global_rand * (global_best_weights[i] - layer)\n",
        "            new_velocities[i] = new_v\n",
        "\n",
        "        self.velocities = new_velocities\n",
        "        \n",
        "        \n",
        "    def _update_weights(self, depth):\n",
        "      old_weights = self.model.get_weights()\n",
        "      new_weights = [None] * len(old_weights)\n",
        "      for i, layer in enumerate(old_weights):\n",
        "        if i>= depth:\n",
        "          new_weights[i] = layer\n",
        "          continue\n",
        "        new_w = layer + self.velocities[i]\n",
        "        new_weights[i] = new_w\n",
        "    \n",
        "\n",
        "      self.model.set_weights(new_weights)\n",
        "\n",
        "    def step(self, x, y, global_best_weights,depth=None):\n",
        "        if depth is None:\n",
        "            depth = self.length\n",
        "        self._update_velocities(global_best_weights, depth)\n",
        "        self._update_weights(depth)\n",
        "        return self.get_score(x, y)\n",
        "\n",
        "    def get_best_weights(self):\n",
        "        return self.best_weights\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pZ4e84Z7XNI"
      },
      "outputs": [],
      "source": [
        "class ProgressBar:\n",
        "    def __init__(self, steps, updates=1):\n",
        "        self.step = 100\n",
        "        self.step_size = (steps // updates) + 1\n",
        "        self.total_steps = steps\n",
        "        self.updates = updates\n",
        "\n",
        "        bar = self._make_bar(0)\n",
        "        print(bar, end=' ')\n",
        "\n",
        "    def update(self, i):\n",
        "        if i % self.step_size > 0:\n",
        "            return\n",
        "\n",
        "        self.step = i // self.step_size\n",
        "        bar = self._make_bar(i)\n",
        "\n",
        "        print(bar, end=' ')\n",
        "\n",
        "    def done(self):\n",
        "        self.step = self.total_steps\n",
        "        bar = self._make_bar(self.updates)\n",
        "        print(bar)\n",
        "\n",
        "    def _make_bar(self, x):\n",
        "        bar = \"[\"\n",
        "        for x in range(self.updates):\n",
        "            print(\"\\r\", end=' ')\n",
        "            bar += \"=\" if x < self.step else \" \"\n",
        "        bar += \"]\"\n",
        "        return bar\n",
        "\n",
        "# tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQY_g0AN7agj"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "class Optimizer:\n",
        "    def __init__(self, model, loss,\n",
        "                 n = 150,\n",
        "                 acceleration=3,\n",
        "                 local_rate=2.5,      #1.2\n",
        "                 global_rate=3):  #1.6\n",
        "\n",
        "        self.n_particles = n\n",
        "        self.structure = model #.to_json()\n",
        "        self.particles = [None] * n\n",
        "        self.loss = loss\n",
        "        self.length = len(model.get_weights())\n",
        "        self.history = []\n",
        "        \n",
        "\n",
        "        params = {'acc': acceleration, 'local_acc': local_rate, 'global_acc': global_rate}\n",
        "\n",
        "        for i in range(n-1):\n",
        "            m = model #_from_json(self.structure)\n",
        "                 #keras.models.       \n",
        "            m.compile(loss=loss,optimizer='Adam')\n",
        "            self.particles[i] = Particle(m, params)\n",
        "\n",
        "        self.particles[n-1] = Particle(model, params)\n",
        "\n",
        "        self.global_best_weights = None\n",
        "        self.global_best_score = BIG_SCORE\n",
        "\n",
        "    def fit(self, x, y, steps=100, batch_size=4):\n",
        "        num_batches = x.shape[0] // batch_size\n",
        "\n",
        "        for i, p in enumerate(self.particles):\n",
        "            local_score = p.get_score(x, y)\n",
        "\n",
        "            if local_score < self.global_best_score:\n",
        "                self.global_best_score = local_score\n",
        "                self.global_best_weights = p.get_best_weights()\n",
        "\n",
        "        print(\"PSO -- Initial best score {:0.4f}\".format(self.global_best_score))\n",
        "\n",
        "        bar = ProgressBar(steps, updates=20)\n",
        "\n",
        "        for i in range(0,steps):\n",
        "            print('i {}'.format(str(i)) )\n",
        "            for j in range(0,num_batches):\n",
        "                \n",
        "                x_ = x[j*batch_size:(j+1)*batch_size,:]\n",
        "                y_ = y[j*batch_size:(j+1)*batch_size]\n",
        "                \n",
        "                \n",
        "                \n",
        "                for i in range(0,len(p.velocities)):    #NEW MODIFICATION\n",
        "                  MIN1 = p.velocities[i] <= -1000;\n",
        "                  MAX1 = p.velocities[i] >= 1000;\n",
        "\n",
        "                if not any(MIN1) and not any(MAX1):     #Min_VAl & Max_VAL @- NEW MODIFICATION \n",
        "\n",
        "                  for p in self.particles:\n",
        "                    local_score = p.step(x_, y_, self.global_best_weights)\n",
        "\n",
        "                    if local_score < self.global_best_score:\n",
        "                        self.global_best_score = local_score\n",
        "                        self.global_best_weights = p.get_best_weights()\n",
        "              \n",
        "                self.history.append(self.global_best_score)\n",
        "                \n",
        "                \n",
        "            bar.update(i)\n",
        "        for i, p in enumerate(self.particles):\n",
        "          local_score = p.get_score(x, y)\n",
        "\n",
        "          if local_score < self.global_best_score:\n",
        "            self.global_best_score = local_score\n",
        "            self.global_best_weights = p.get_best_weights()\n",
        "\n",
        "        print(\"PSO -- Mean square error {:0.4f}\".format(self.global_best_score))\n",
        "      \n",
        "        bar.done()\n",
        "  \n",
        "    def get_best_model(self):\n",
        "        best_model = model_s #_from_json(self.structure)\n",
        "      #keras.models.\n",
        "        best_model.set_weights(self.global_best_weights)\n",
        "        best_model.compile(loss=self.loss,optimizer='Adam') #self.loss\n",
        "        return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V90oBhdA7eTj"
      },
      "outputs": [],
      "source": [
        "#Store the data set\n",
        "file_id = '1oQw9vdiTsmsvOayjpskpVj_ZvBLPCVII' # file key of the relevant google sheet\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id}) \n",
        "downloaded.GetContentFile('data.xlsx')   \n",
        "\n",
        "df = pd.read_excel('data.xlsx')\n",
        "\n",
        "dataset = df.values                       #Convert the dataframe into an array\n",
        "inputs  = dataset[1:212,0:4]                #end - 1  \n",
        "targets = dataset[1:212,4:5]               #end\n",
        "\n",
        "#How to split this into train & test & val data?\n",
        "X_train, x_test, Y_train, y_test = train_test_split(inputs, targets, test_size=0.2, random_state = 4) \n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvfPdz9640Ie"
      },
      "outputs": [],
      "source": [
        "# way to standardize the values to common scale.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(x_train)\n",
        "X_test = sc.fit_transform(x_test)\n",
        "Y_train = sc.fit_transform(y_train)\n",
        "Y_test = sc.fit_transform(y_test)\n",
        "INPUTS = sc.fit_transform(inputs)\n",
        "TARGETS = sc.fit_transform(targets)\n",
        "X_val = sc.fit_transform(x_val)\n",
        "Y_val = sc.fit_transform(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k68wn-7H7h19"
      },
      "outputs": [],
      "source": [
        "N = 150     # number of particles\n",
        "STEPS = 15  # number of steps\n",
        "LOSS = 'mse'# Loss function\n",
        "BATCH_SIZE = 2  # Size of batches to train on\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLMCVSIIdXpv"
      },
      "outputs": [],
      "source": [
        "#close the previous model variable\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hDMAv6_fqDN"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers\n",
        "from keras.optimizers import adam_v2, adagrad_v2, adamax_v2\n",
        "def build_model():\n",
        "    \"\"\"\n",
        "    Builds test Keras model for predicting Iris classifications\n",
        "    :param loss (str): Type of loss - must be one of Keras accepted keras losses\n",
        "    :return: Keras dense model of predefined structure\n",
        "    \"\"\"\n",
        "    #Voila, now letsd build Neural network model\n",
        "    model = Sequential([                                              \n",
        "                    Dense(16, activation = 'sigmoid', input_shape=(4, )),\n",
        "                    Dense(48, activation = 'relu'),                   \n",
        "                    Dense(16, activation = 'tanh') \n",
        "                  ])\n",
        "    model.add(Dense(1,))  \n",
        "\n",
        "    model.compile(loss='mse', optimizer = adam_v2.Adam(learning_rate=0.01) , metrics=['mae'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWZyxjrs7oPO"
      },
      "outputs": [],
      "source": [
        "def vanilla_backpropagation(X_train, Y_train, X_val, Y_val):\n",
        "    \"\"\"\n",
        "    Runs N number of backpropagation model training simulations\n",
        "    :param x_train: x values to train on\n",
        "    :param y_train: target labels to train with\n",
        "    :return: best model run as measured by LOSS\n",
        "    \"\"\"\n",
        "    best_model = None\n",
        "    best_score = 100.0\n",
        "    N = 150\n",
        "    for i in range(N):\n",
        "        model_s = build_model()\n",
        "        model_s.fit(X_train, Y_train, validation_data = (X_val,Y_val),\n",
        "                    epochs=60,\n",
        "                    batch_size=2,\n",
        "                    verbose=0)\n",
        "        train_score = model_s.evaluate(X_train, Y_train, batch_size=BATCH_SIZE, verbose=0)\n",
        "        #print(model_s.metrics_names)\n",
        "        for i in train_score:\n",
        "          if i < best_score:\n",
        "            best_model = model_s\n",
        "            best_score = i\n",
        "    print(\"Mean absolute Score = \", best_score)\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eSFCF9JPgWGQ"
      },
      "outputs": [],
      "source": [
        "#Main function part 1\n",
        "LOSS = 'mse'\n",
        "BATCH_SIZE = 2\n",
        "model_s = vanilla_backpropagation(X_train, Y_train, X_val, Y_val)\n",
        "# Instantiate optimizer with model, loss function, and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eeD42ZZcU2r2"
      },
      "outputs": [],
      "source": [
        "#Main function part 2\n",
        "pso = Optimizer(model=model_s,\n",
        "loss=LOSS,\n",
        "n=N,                # Number of particles\n",
        "acceleration=3,     # Contribution of recursive particle velocity (inertia)\n",
        "local_rate=2.5,     # Contribution of locally best weights to new velocity\n",
        "global_rate=3)      # Contribution of globally best weights to new velocity\n",
        "\n",
        "# Train model on provided data\n",
        "pso.fit(X_train, Y_train, steps=STEPS, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PoYo9jpuNHTK",
        "outputId": "ecbae031-baaa-4eb0-af92-eb587519d4b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PSO -- train: 0.0033  test: 0.0451  val:  0.0693\n"
          ]
        }
      ],
      "source": [
        "#Main function part 3\n",
        "# Get a copy of the model with the globally best weights\n",
        "model_p = pso.get_best_model()\n",
        "\n",
        "p_train_score = model_p.evaluate(X_train, Y_train, batch_size=BATCH_SIZE, verbose=0)\n",
        "p_test_score = model_p.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "p_val_score = model_p.evaluate(X_val, Y_val, batch_size=BATCH_SIZE, verbose=0)\n",
        "print(\"PSO -- train: {:.4f}  test: {:.4f}  val:  {:.4f}\".format(p_train_score, p_test_score, p_val_score))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6HCFPLsRu7Y4"
      },
      "outputs": [],
      "source": [
        "#way to extract weights from the Neural network\n",
        "#for lay in model_p.layers:\n",
        " #   print(lay.name)\n",
        "  #  print(lay.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uB5Mv2TYNsEs"
      },
      "outputs": [],
      "source": [
        "# inverse transform\n",
        "y_pred = model_p.predict(INPUTS)\n",
        "y_pred_inv = sc.inverse_transform(y_pred)\n",
        "\n",
        "y_predtrain = model_p.predict(X_train)\n",
        "y_predtrain_inv = sc.inverse_transform(y_predtrain)\n",
        "\n",
        "y_predtest = model_p.predict(X_test)\n",
        "y_predtest_inv = sc.inverse_transform(y_predtest)\n",
        "\n",
        "y_predval = model_p.predict(X_val)\n",
        "y_predval_inv = sc.inverse_transform(y_predval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bymvcWoyr-Kv",
        "outputId": "19c6417c-3a39-454c-c5a3-c05dea1cb295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4964567142233682\n",
            "1.2232974757692294\n"
          ]
        }
      ],
      "source": [
        "# to find MSE of overall data\n",
        "def mse(actual, pred): \n",
        "    actual, pred = np.array(actual), np.array(pred)\n",
        "    return np.square(np.subtract(actual,pred)).mean()\n",
        "print(mse(targets, y_pred_inv))\n",
        "\n",
        "def rmse(actual, pred): \n",
        "    actual, pred = np.array(actual), np.array(pred)\n",
        "    return np.sqrt(np.square(np.subtract(actual,pred)).mean())\n",
        "print(rmse(targets, y_pred_inv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xVwTfGOaijrv",
        "outputId": "108354fb-46e8-45c5-8c32-b513052af8e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "r2 Train 0.9967048433785978\n",
            "r2 Test 0.9549252460475335\n",
            "r2 validation 0.9306851556751203\n",
            "r2 overall 0.9941424948082406\n"
          ]
        }
      ],
      "source": [
        "# calculating R2 values \n",
        "from sklearn.metrics import r2_score\n",
        "r2_all1 = r2_score(TARGETS, y_pred)\n",
        "r2_test1 = r2_score(Y_test, y_predtest)\n",
        "r2_train1 = r2_score(Y_train, y_predtrain)\n",
        "r2_val1 = r2_score(Y_val, y_predval)\n",
        "print ('r2 Train',r2_train1)\n",
        "print ('r2 Test',r2_test1)\n",
        "print ('r2 validation',r2_val1)\n",
        "print ('r2 overall',r2_all1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iT5BxymdvRxq"
      },
      "outputs": [],
      "source": [
        "# plotting training fit\n",
        "z=[]\n",
        "a=[]\n",
        "e=[]\n",
        "\n",
        "for i in range(0,len(y_train)):\n",
        "  z.append(x_train[i][1])\n",
        "  a.append(y_train[i][0])\n",
        "  e.append(y_predtrain_inv[i][0])\n",
        "r=np.array(z)\n",
        "n=np.array(a)\n",
        "t = np.array(e)\n",
        "\n",
        "plt.scatter(a,e)\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot(np.unique(a), np.poly1d(np.polyfit(a, e, 1))(np.unique(a)))\n",
        "\n",
        "plt.text(0.6, 0.5, 'R-squared = %0.2f' % r2_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IW5y-wJCvSmx"
      },
      "outputs": [],
      "source": [
        "# plotting training fit\n",
        "z=[]\n",
        "a=[]\n",
        "e=[]\n",
        "\n",
        "for i in range(0,len(y_val)):\n",
        "  z.append(x_val[i][1])\n",
        "  a.append(y_val[i][0])\n",
        "  e.append(y_predval_inv[i][0])\n",
        "r=np.array(z)\n",
        "n=np.array(a)\n",
        "t = np.array(e)\n",
        "\n",
        "plt.scatter(a,e)\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot(np.unique(a), np.poly1d(np.polyfit(a, e, 1))(np.unique(a)))\n",
        "\n",
        "plt.text(0.6, 0.5, 'R-squared = %0.2f' % r2_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4JmBQTsJvZ0n"
      },
      "outputs": [],
      "source": [
        "# plotting testing fit\n",
        "z=[]\n",
        "a=[]\n",
        "e=[]\n",
        "\n",
        "for i in range(0,len(y_test)):\n",
        "  z.append(x_test[i][1])\n",
        "  a.append(y_test[i][0])\n",
        "  e.append(y_predtest_inv[i][0])\n",
        "r=np.array(z)\n",
        "n=np.array(a)\n",
        "t = np.array(e)\n",
        "\n",
        "plt.scatter(a,e)\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot(np.unique(a), np.poly1d(np.polyfit(a, e, 1))(np.unique(a)))\n",
        "plt.text(0.6, 0.5, 'R-squared = %0.2f' % r2_train)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PSO-ANN code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}